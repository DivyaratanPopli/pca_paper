\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage[super]{nth}
\usepackage{tabularx}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{lineno}
\usepackage[
singlelinecheck=false
]{caption}
\usepackage{amsmath}
\usepackage{amsfonts}
%\usepackage{bm}
%\usepackage{bbm}
\usepackage{graphicx}
\usepackage{csvsimple}
%\usepackage[section]{placeins}
\usepackage{lineno}
\usepackage{natbib}
\usepackage[T1]{fontenc}
\usepackage[breaklinks]{hyperref}
%\usepackage{microtype}
\usepackage{booktabs}
\usepackage{array}



\title{A joint framework for the estimation of PCA and F-statistics using ancient DNA}
\author{Divyaratan Popli, Benjamin M. Peter}
%\date{5 August 2021}
\linenumbers

\setlength{\parskip}{1em}
\setlength{\parindent}{0em}

\newcommand{\BY}{\mathbf{Y}}
\newcommand{\BW}{\mathbf{W}}
\newcommand{\BU}{\mathbf{U}}
\newcommand{\BI}{\mathbf{I}}
\newcommand{\BZ}{\mathbf{Z}}
\newcommand{\BD}{\mathbf{D}}
\newcommand{\BN}{\mathbf{N}}
\newcommand{\BH}{\mathbf{H}}
\newcommand{\BP}{\mathbb{P}}
\newcommand{\Btheta}{\pmb{\theta}}


\newcommand{\MX}{\mathbf{X}}
\newcommand{\MS}{\mathbf{S}}
\newcommand{\MP}{\mathbf{P}}
\newcommand{\MG}{\mathbf{G}}
\newcommand{\MZ}{\mathbf{Z}}
\newcommand{\ML}{\mathbf{L}}
\newcommand{\COV}[2]{\text{Cov}(#1,#2)}

\newcommand{\CX}{\mathcal{X}}


\hbadness=99999

\usepackage{breqn}

\begin{document}

\maketitle


\begin{abstract}

\noindent Principal component analysis (PCA) and $F$-statistics are routinely used in population genetic and archaeogenetic studies. However, these are closely related analyses and reveal the same biological signal. Here, we present a statistical framework to combine them into a joint analysis. In particular, we discuss the differences of probabilistic PCA, Latent Subspace Estimation and classical PCA, and show that $F$-statistics are more naturally interpreted in a probabilistic PCA framework. We also show that individual-based $F$-statistics can be accurately estimated from probabilistic PCA in the presence of large amounts of missing data. We compare estimates from probabilistic PCA-based framework to ADMIXTOOLS 2 using simulations and published data, and show that this joint estimation framework addresses limitations of estimating F-statistics and PCA independently.

\end{abstract}

\section{Introduction}

Most populations live in heterogeneous and changing environments, and thus will exhibit some population structure, which we expect to change over time. Over short time scales, there are two principal forces affecting this structure. First is genetic drift, which increases differentiation between populations over time due to isolation. Second, secondary contact between isolated populations causes intermediate genotypes and thus reduces differentiation. For the purpose of this paper, we treat the terms gene flow, admixture and migration synonymously. A common goal in population genetics is to characterize the genetic variation caused by these processes.

In particular for humans, there has been a long-standing debate on how we conceptualize genetic population structure, whether populations as such exist, or to what degree they are the result of biased sampling designs \citep{serre_evidence_2004, rosenberg_clines_2005, peter_genetic_2020}, and how they affect ancestry estimation \citep{mathieson_what_2020, simon_contribution_2023} and impact association studies \citep{price_principal_2006}. Questions like these are of fundamental importance because they impact equitable access to genetic medicine \citep{popejoy_genomics_2016}, how we think about race in the context of genetics \citep{lewontin_apportionment_1972, novembre_background_2022} and other (mis)uses of genetic variation.


\subsection{Overview of methods to study admixture}
There is a large array of approaches and methods available to make inferences about population structure (review by \cite{schraiber_methods_2015}), aiming at different time scales, making different modelling assumptions or treating data differently. 
One wide class of methods are global ancestry methods. They summarize the entirety of the genome into a small number of summary statistics, with the idea that different genomic loci are (pseudo-) independent replicates of the historical process \citep{pritchard_inference_2000, gopalan_scaling_2016, patterson_ancient_2012,alexander_fast_2009, tang_estimation_2005}. Global ancestry methods stand in contrast to local ancestry methods, that use an ancestral recombination graph or an approximation thereof, to infer the detailed ancestry of each locus  \citep{lawson_inference_2012, hellenthal_genetic_2014, speidel_method_2019, kelleher_inferring_2019}. Global ancestry methods are widely used because they tend to be much simpler and easier to interpret than local ancestry methods, and are sufficient for many applications \citep{pritchard_inference_2000, patterson_population_2006}.

For large datasets with dozens of individuals spanning a wide range of sampling locations, we can further classify global methods into joint analyses that use data from all individuals in a multivariate framework, such as Principal Component Analysis (PCA) \citep{cavalli-sforza_analysis_1975, patterson_population_2006, novembre_genes_2008}, structure \citep{pritchard_inference_2000}, ADMIXTURE \citep{alexander_fast_2009}, and summary-statistic-based approaches relying on statistics that include two or a small number of populations (e.g. $F_{ST}$-based or site-frequency-spectrum based methods), and use large numbers of these summaries to build more complex models \citep{excoffier_fastsimcoal_2011, kamm_efficiently_2020, gutenkunst_inferring_2009}. 

\subsection{$F$-statistics}
A popular framework based on summary statistics, particularly in studies of ancient human populations \citep{orlando_ancient_2021}, relies on a set of statistics called $F$-statistics \citep{patterson_ancient_2012, peter_admixture_2016}. As we will define in full detail in the theory section, $F$-statistics measure the genetic drift shared between two, three, or four populations \citep{patterson_ancient_2012, peter_admixture_2016}.  These patterns of shared drift are then compared to a null model corresponding to a population tree connecting the sampled individuals. Gene flow between distinct populations violates the assumption of treeness, and causes a deviation from the null hypothesis. Thus, $F$-statistics form the basis of an intuitive and powerful framework to test hypotheses of admixture (Fig. \ref{fig2:overview}). 

Despite their name, $F$-statistics should be thought of as parameters, that are defined in terms of \textit{population} allele frequencies. Since we typically only have genotype data from a small subsample of individuals, the population allele frequencies are unobserved, and $F$-statistics must be estimated from \textit{sample} allele frequencies. This estimation is not trivial: Patterson et al. showed that a naive estimator would be biased, and introduced a bias-correction term \citep{patterson_ancient_2012}. This bias is largest if the sample size is small (e.g. for single genomes), and will reduce in magnitude for larger samples (see eq. \ref{eq:f2_correction}). 

To do inference, we use combinations of statistics that include two, three or four populations. Thus, treating each individual independently would give us the potential to compute more statistics, and thus the highest resolution representation of the underlying population structure. However, large numbers of statistics will be harder to interpret, and would have relatively low statistical accuracy. 

Thus, samples are typically grouped into as large populations as possible, to improve statistical accuracy and to make interpretation easier. This creates a trade-off between a fine-scale view of population structure with low statistical accuracy, and a coarser-scale representation that has the danger of an overly simplistic view of the genetic structure of the studied species.


This issue is compounded by missing data: In ancient DNA, the amount of preserved DNA is often a limiting factor, and hence low-coverage genomes, and heterogeneous data quality are the norm \citep{orlando_ancient_2021}. In studies with dozens of individuals, variable levels of missingness add additional statistical noise that make individual-based statistics even less accurate, leading to a larger pressure to group individuals, and hence simplify the population structure. 

To rectify this trade-off between analysis resolution and statistical accuracy, we use the theoretical results linking PCA and $F$-statistics from \cite{peter_geometric_2022} to develop a PCA-based framework that jointly estimates $F$-statistics between all sets of individuals in a large data set. This framework has the advantage that it does not require \textit{a priori} assignment of individuals to populations, and allows for the imputation of data missing at random. These advances allow for accurate and finely-grained analyses of population structure.

\subsection{PCA}
PCA is one of the most widely used global ancestry methods to uncover population structure \citep{cavalli-sforza_analysis_1975, mcvean_genealogical_2009, engelhardt_analysis_2010}. PCA has the advantage that it makes minimal assumptions on the underlying data, and thus can be applied flexibly even when little is known about the underlying patterns of genetic variation. The key empirical feature of PCA is that it tends to cluster similar individuals nearby, and provides an easy to understand and often useful visualization of the genetic variation in the data. PCA is also widely used to model population structure in association studies, where structure is an undesired covariate that has to be regressed out \citep{price_principal_2006}. The big caveat is that it can be difficult to interpret PCAs, since there is no underlying mechanistic model, nor does it lend itself to model comparisons or formal tests of admixture \citep{mcvean_genealogical_2009, novembre_interpreting_2008}.


PCA was introduced and popularized as a tool to study human genetic structure by Cavalli-Sforza et al., and using just a handful of genetic loci, they were able to use PCA to accurately describe patterns of human genetic variation, and to make inference about their possible causes, although the way these patterns were analyzed were typically qualitative \citep{menozzi_synthetic_1978, sforza_great_1995, l_l_cavalli-sforza_history_1996}.

As is still the norm with $F$-statistics, Cavalli-Sforza aggregated individuals into populations before performing PCA. With the advent of genomic data, PCA methods became superseded by individual-based approaches, which have higher resolution, and do not require grouping of individuals into populations \citep{patterson_population_2006, novembre_genes_2008, price_principal_2006}.

In the rest of this chapter, we refer to the conventional method of visualizing the variance of the data in lower dimensions as classical PCA to differentiate it from other PCA methods such as Probabilistic PCA and Latent Subspace Estimation.

\subsection{Probabilistic PCA and Latent Subspace Estimation (LSE)}
While classical PCA helps in visualizing the overall variation in the data, it does not differentiate sources of variation. In population genetic studies, it is often desirable to tease apart the biological variation that is due to the shared population history (leading to the \textit{population} allele frequency), and the statistical variation that is due to missing data, or limited sampling. Analogous to $F$-statistics, classical PCA suffers from the issue that the population structure estimator is biased when sample allele frequencies are used instead of population allele frequencies.

Probabilistic PCA (PPCA) is an extension of classical PCA that aims to rectify this by incorporating an explicit probabilistic framework \citep{tipping_probabilistic_1999-1}. The key idea is to decompose the variation in the data into a (low-rank) covariance matrix that models population structure, and a diagonal matrix that captures variation due to sampling.

PPCA was introduced by \citep{tipping_probabilistic_1999-1} in a Gaussian setting, where the sampling error is modelled as normally distributed, and each individual is assumed to have the same sampling error (homoskedastic noise). However, in population genetics, genotypes are discrete and thus the sampling errors are binomially distributed. Furthermore, individuals coming from populations with different effective population sizes will have different heterozygosities resulting in heteroskedastic noise. Thus a more sophisticated model that assumes different binomial sampling noise for each individual has been implemented in Latent Subspace Estimation (LSE) \citep{chen_consistent_2015, van_waaij_evaluation_2023, cabreros_likelihood-free_2019}. we give a detailed description of both frameworks in sections \ref{theory-ppca} and \ref{theory-lse}. In summary, the main difference between classical PCA, PPCA and LSE is in the way they model the noise in the observed data due to sampling (see Fig.\ref{fig1:pca_ppca}). 

\begin{figure}[ht!]
    \includegraphics[width=16.5cm]{Images/Figures/pca_all_genetic.png}
    \centering
    \caption{Comparison of classical PCA, PPCA and LSE: we simulated genotypes of 104 individuals belonging to 10 different populations, and compared the top eigenvalues obtained from different PCA methods.}
    \label{fig1:pca_ppca} 
\end{figure}

\subsection{Classical PCA and $F$-statistics}
A common analysis paradigm in ancient DNA is to use classical PCA for exploratory and descriptive analyses, and then follow them up with methods based on $F$-statistic for a more formal treatment (typically in a third step, methods that synthesize many $F$-statistics are also applied, although we will not cover those here) \citep{orlando_ancient_2021}. Because they were developed independently and occur at different stages of the analyses, classical PCA and $F$-statistics use different data groupings and different normalizations, and are usually not quantitatively compared. 

A study by \cite{oteo-garcia_geometrical_2021} showed that $F$-statistics and the hypothesis testing for admixture can be interpreted geometrically in a multi-dimensional allele frequency space. Since classical PCA only rotates the data points to the axes of maximum variation, \cite{peter_geometric_2022} extended this result to show that $F$-statistics can be interpreted geometrically in the context of classical PCA. The study concluded that $F$-statistics and classical PCA are closely related, and $F$-statistics can thus be used to interpret PCA plots; genetic drift will move individuals further apart from each other on a PCA-plot. Independent drift, i.e. unconnected populations, will drift on orthogonal axes in PCA-space. On the other hand, admixed individuals will be placed between their populations of origin, and these placements can be measured using $F$-statistics.  

However, while \citep{peter_geometric_2022} develops a theoretical link between PCA and $F$-statistics, it does not deal with statistical uncertainty, and thus cannot easily be applied to noisy data. 

Here, we develop a statistical framework to jointly estimate PCA and $F$-statistics. we show how in particular the choice of the PCA-algorithm (classical PCA, PPCA, LSE) greatly impacts the results. With simulations, we show that $F$-statistics estimated from PPCA-based framework are more accurate compared to that estimated with naive estimators, when there is low sample size and missing data. we also draw comparison between PPCA-based framework and ADMIXTOOLS 2 using published Neanderthal samples. Our approach improves the estimation of $F$-statistics, and leads to some natural suggestions about how and when different PCA methods should be used.

\section{Theory}\label{theory}
In this section, we give a more detailed and formal overview of $F$-statistics and PCA, and their various estimators. we show that the model underlying $F$-statistics is very similar to that of PPCA, and identical to that of LSE. These findings will then be substantiated in the results section using simulated and real data.

\subsection{$F$-statistics}\label{fstats}
I follow the original notation of \cite{patterson_ancient_2012}, and distinguish between the \textit{parameters} $F_2$, $F_3$ and $F_4$, and their \textit{estimators} from empirical data, denoted by lower-case $f_2$, $f_3$ and $f_4$. The three $F$-statistics are defined in terms of population allele frequencies as follows:

\begin{align}\label{eq:f_intro}
F_2(X_1,X_4) &= \frac{1}{S}\sum_{s=1}^S(\mathcal{X}_{1s} - \mathcal{X}_{4s})^2\nonumber\\
F_3(X_1;X_3,X_4) &= \frac{1}{S}\sum_{s=1}^S(\mathcal{X}_{1s} - \mathcal{X}_{3s})(\mathcal{X}_{1s} - \mathcal{X}_{4s})\nonumber\\
F_4(X_1,X_2;X_3,X_4) &= \frac{1}{S}\sum_{s=1}^S(\mathcal{X}_{1s} - \mathcal{X}_{2s})(\mathcal{X}_{3s} - \mathcal{X}_{4s}).\nonumber\\
\end{align}

Here, $S$ is the total number of SNPs, and $\mathcal{X}_{is}$ is the (unobserved) population allele frequency in population $X_i$ at SNP $s$. 

Assuming a tree-like relationship between populations, $F_2(X_1,X_4)$ is interpreted as the branch length between populations $X_1$ and $X_4$ (Fig. \ref{fig2:overview} A) and it reflects the expected amount of drift that occurred between $X_1$ and $X_4$. $F_3(X_1;X_3,X_4)$ represents the amount of drift that occurred on the external branch connecting $X_1$ to the common ancestor node of $X_3$ and $X_4$ (Fig. \ref{fig2:overview} B). Under a tree-like model, $F_3$ will always be non-negative. However, in the case where $X_1$ is admixed between $X_3$ and $X_4$, $F_3(X_1;X_3,X_4)$ may be negative, and hence this is used as a test for admixture \citep{peter_admixture_2016, patterson_ancient_2012}. $F_4(X_1,X_4;X_2,X_3)$ represents the covariance between shared drifts between X1,X4 and X2,X3. This would be represented by the internal branch between the common ancestor nodes of $X_1$, $X_2$ and $X_3$, $X_4$ (Fig. \ref{fig2:overview} C). $F_4$- statistic, with a different permutation of the populations, can be used as test of admixture. $F_4(X_1,X_2;X_3,X_4)$ is expected to be 0 if $X_1$, $X_2$, $X_3$, $X_4$ are related to each other by a tree (Fig. \ref{fig2:overview} D). In this case, a significantly non-zero value suggests a departure from the null model of treeness.

It is straightforward to verify that $F_3$ and $F_4$ can be written in terms of $F_2$s as:

\begin{align}\label{eq:f3_f4}
F_3(X_1;X_3,X_4) &= \frac{1}{2} [F_2(X_1,X_3) + F_2(X_1,X_4) - F_2(X_3,X_4)]\nonumber\\
F_4(X_1,X_2;X_3,X_4) &= \frac{1}{2} [F_2(X_1,X_3) + F_2(X_2,X_4) - F_2(X_1,X_4) - F_2(X_2,X_3)]\text{.}
\end{align}

Hence, in practice, all $F$-statistics can be calculated from linear combinations of $F_2$. Patterson et al. showed that the naive application of eq. \ref{eq:f_intro} to \emph{sample} allele frequency data will be biased, particularly when the sample size is small. They thus introduced the bias-corrected estimator \citep{ patterson_ancient_2012}:


\begin{align}\label{eq:f2_correction}
f_2(X_1,X_4) &= \frac{1}{S}\sum_{s=1}^S\left[(x_{1s} - x_{4s})^2 - \frac{x_{1s}(1-x_{1s})}{n_{1s}-1} - \frac{x_{4s}(1-x_{4s})}{n_{4s}-1}\right] \text{.}
\end{align}\label{eq:f2_error}

Here, we denote the sample allele frequency for population $X_i$ at SNP $s$ as $x_{is}$, and the number of non-missing haploids in population $X_i$ at SNP $s$ as $n_{is}$. When only a single (diploid) individual is sampled from population $X_i$, $n_{is}=2$ and so this equation still works in this case. However, for (pseudo-) haploid samples $n_{is}=1$ and the denominators are zero. Thus, the unbiased estimators do not exist for single pseudohaploid samples.

Using eq. \ref{eq:f3_f4}, we can see that this sampling bias also affects the calculation of $f_3$,  but the correction terms cancel out for $f_4$, and the equations for $f_4$ and $F_4$ coincide:

\begin{align}\label{eq:f3_correction}
f_3(X_1;X_3,X_4) &=\frac{1}{S} \sum_{s=1}^S\left[(x_{1s} - x_{3s})(x_{1s} - x_{4s}) - \frac{x_{1s}(1-x_{1s})}{n_{1s}-1}\right]
\end{align}\label{eq:f3_error} 

\begin{align}\label{eq:f4_correction}
f_4(X_1,X_2;X_3,X_4) &= \frac{1}{S}\sum_{s=1}^S(x_{1s} - x_{2s})(x_{3s} - x_{4s}) \text{.}
\end{align}\label{eq:f4_error} 

\cite{oteo-garcia_geometrical_2021} showed that $F$-statistics can be defined more generally in a geometrical framework. In this framework, each population can be represented as a point in a high-dimensional allele frequency space. $F_2(X_1, X_4)$ is then the  squared Euclidean distance between points representing populations $X_1$ and $X_4$ (Fig. \ref{fig2:overview} E). $F_3(X_1;X_3,X_4)$ is then a dot product of the vectors $\Vec{\CX1} - \Vec{\CX_3}$ and $\Vec{\CX_1} - \Vec{\CX_4}$ (Fig. \ref{fig2:overview} F). $F_4(X_1,X_4;X_2,X_3)$ is a dot product of vectors $\Vec{\CX_1} - \Vec{\CX_4}$ and $\Vec{\CX_2} - \Vec{\CX_3}$ (Fig. \ref{fig2:overview} G), and $F_4(X_1,X_2;X_3,X_4)$ is a dot product of vectors $\Vec{\CX_1} - \Vec{\CX_2}$ and $\Vec{\CX_3} - \Vec{\CX_4}$ (Fig. \ref{fig2:overview} H).  we define the vector of allele frequencies of population $i$ as $\mathcal{X}_i = (\mathcal{X}_{i1}, \dots, \mathcal{X}_{iS})$. 

\begin{align}\label{eq:f_geometric}
F_2(X_1,X_4) &= \frac{1}{S}||\Vec{\mathcal{X}_{1}} - \vec{\mathcal{X}_{4}}||^2\nonumber\\
F_3(X_1;X_3,X_4) &= \frac{1}{S} \langle\vec{\mathcal{X}_{1}} - \vec{\mathcal{X}_{3}},\vec{\mathcal{X}_{1}} - \vec{\mathcal{X}_{4}}\rangle\nonumber\\
F_4(X_1,X_2;X_3,X_4) &= \frac{1}{S}\langle\vec{\mathcal{X}_{1}} - \vec{\mathcal{X}_{2}},\vec{\mathcal{X}_{3}} - \vec{\mathcal{X}_{4}}\rangle\nonumber\\
\end{align}

Crucially however, these interpretations only hold for the (unobserved) population allele frequencies, but not for the (observed) sample allele frequencies and thus, they cannot be directly applied to data.


\begin{figure}[ht!]
    \includegraphics[width=16.5cm]{Images/Figures/overview.png}
    \centering
    \caption{Schematics showing different interpretations of $F$-statistics. The columns represent $F_2(X_1,X_4)$, $F_3(X_1;X_3,X_4)$, $F4(X_1,X_4;X_2,X_3)$, $F4(X_1,X_2,X_3,X_4)$. The first row shows a tree interpretation of each statistic, the second row shows $F$-statistics in an allele-frequency space with three axes representing three SNPs, and the last row is the interpretation of $F$-statistics on a PCA. Blue lines represent the statistic, and the dotted lines represent orthogonal projections. Black squares denote right angles.}
    \label{fig2:overview}
\end{figure}


\subsection{Classical PCA and $F$-statistics}\label{theory-pca-fstats}

This geometric framework provides a complementary way to understand the properties of $F$-statistics \citep{oteo-garcia_geometrical_2021}. However, many population genetic studies use a large number of SNPs (in the order of a million), and it is not possible to visualize population vectors in such a high dimensional space. Peter at al. showed that one can do dimensionality reduction on such datasets with classical PCA, and use the top PCs to estimate $F$-statistics efficiently \citep{peter_geometric_2022}.

I illustrate this in Fig. \ref{fig2:overview}:  $F_2(X_1,X_4)$ can be thought of as the squared Eucledian distance between populations $X_1$ and $X_4$ in PCA-space. Similarly, $F_3(X_1;X_3,X_4)$ is represented as the length of projection of the vector $X_1-X_3$ on $X_1-X_4$. Internal branch length $F_4(X_1,X_4;X_2,X_3)$ can be described as the length of projection of $X_2-X_3$ on $X_1-X_4$ on PCA, and the test of admixture $F_4(X_1,X_2,X_3,X_4)$ is equivalent to the length of projection of $X_1-X_2$ on $X_3-X_4$. 


To formalize the relationship between $F$-statistics and PCA, let us assume our dataset $\mathbf{X}$ has $M$ individuals and $S$ SNPs, such that our allele frequency matrix $\MX$ has the dimension $[M \times S]$. The $i$-th row of $\MX$ corresponds to the $S$-dimensional row-vector $\Vec{\mathcal{X}_i}$ whose entries are  allele frequencies $\in$ [0,1]. Classical PCA of mean-centered $\MX$ allows us to project this $S$-dimensional data onto a $q$ dimensional subspace, where $q < M$. $q = M-1$ represents the case where we retain all the PCs, and thus classical PCA only rotates $\MX$. However, in practice we often only need few PCs ($q \ll M$) to explain most of the variation in the genetic data \citep{peter_geometric_2022}, which can greatly simplify calculations and visualizations. 

A common algorithm to estimate PCs is via Singular Value Decomposition (SVD). For this approach, we first mean-center $\MX$ to a  matrix $\MX_c$ by subtracting the mean genotype $\mu_s$ from all the rows of $\MX$, and then decompose $\MX_c$ into an orthonormal matrix $\mathbf{U}$, a diagonal matrix $\mathbf{E}$, and another orthonormal matrix $\mathbf{V^T}$.

$$\MX_c = (\mathbf{U}\mathbf{E}) \mathbf{V}^T = \mathbf{WL},$$

We can perform SVD to decompose $\MX_c$ into a product of $\mathbf{W} = \mathbf{UE}$ and $\mathbf{L} = \mathbf{V}^T$. In the context of PCA, $\mathbf{W}_{[M\times M]}$ is a matrix of principal components (where the $i$-th column corresponds to the $i$-th PC) and contains information about structure, while $\mathbf{L}_{[M\times S]}$, also known as the SNP loadings contains the contribution of each SNP to each PC \citep{gower_distance_1966}, and can be used to identify outlier SNPs that may be potential candidates for selection \citep{meisner_detecting_2021}. 

Since $F$-statistics can be written as dot products in an allele frequency space (eq. \ref{eq:f_geometric}), and dot products are invariant to rotation, classical PCA will not change $F$-statistics as long as we retain all PCs and we can calculate $F$-statistics from the PCs directly \citep{peter_geometric_2022}:

\begin{align}\label{eq:f_pca}
F_2(X_1,X_4) &= \frac{1}{S}\sum_{s=1}^S(\mathcal{X}_{1s} - \mathcal{X}_{4s})^2\nonumber\\
&= \frac{1}{S}\sum_{s=1}^S\left((\mathcal{X}_{1s} - \mu_s)- (\mathcal{X}_{4s} - \mu_s)\right)^2 = F_2(X_{c1},X_{c4})\nonumber\\
&=\frac{1}{S} \sum_{j=1}^{M}(w_{1j} - w_{4j})^2 = F_2(W_1,W_4)\nonumber\\
\end{align}

A difficulty in the practical application of this result is that the geometric considerations of \cite{oteo-garcia_geometrical_2021} and \cite{peter_geometric_2022} only hold for the (generally unobserved) population allele frequencies, but not for sample allele frequencies. In ancient DNA, PCA is most commonly run directly on individual-level genotype data \citep{patterson_population_2006}, and hence on the biased sample allele frequencies. Thus, applying the classical PCA-based estimator (eq. \ref{eq:f_pca}) to calculate $F$-statistics would likewise result in biased estimate.


\cite{oteo-garcia_geometrical_2021} resolved this by calculating $F$-statistics using populations with large number of individuals and with no missing data \citep{oteo-garcia_geometrical_2021}. In \cite{peter_geometric_2022}, unbiased estimates of the PCA reconstructions were obtained indirectly by first calculating all pairwise $F_2$-statistics, and then performing a multidimensional-scaling decomposition equivalent to classical PCA. 

\subsection{PPCA and $F$-statistics}\label{theory-ppca}
Since PPCA and LSE separate the population variation from sampling uncertainty, we can use them to calculate (approximately) unbiased $f$-statistics.

Here, we implement approaches based on PPCA and LSE that aim to calculate the bias-corrected estimates of $F_2$ from  PCA, by explicitly separating out the sampling error.

The first approach is based on probabilistic PCA (PPCA) \citep{tipping_probabilistic_1999-1, agrawal_scalable_2020}. The simple idea here is to get the covariance matrix for all pairs of samples, along with an error term. We do this by estimating matrix $\BW$, such that $\mathbf{W}\mathbf{W}^T$ denotes the covariance matrix, and $\Psi$ represents the average covariance due to sampling noise. Then, the model fit by PPCA can be written as: 
\begin{equation*}
    \MX_c \sim N(0, \mathbf{W}\mathbf{W}^T + \Psi \BI),
\end{equation*} 
where $N$ denotes a multivariate normal distribution, $\MX_c$ is again the centered genotype matrix, $\mathbf{W}$ is a $M \times q$ matrix of linear mappings, $\BI$ is the identity matrix and $\Psi$ is a scalar noise term. Intuitively, $\mathbf{W}\mathbf{W}^T$ captures the covariance in the observed data, analogous to the $F$-statistics, and $\Psi \BI$ is analogous to the bias-correction term. Since $\Psi$ is a scalar, all entries on the diagonal of $\Psi \BI$ will be the same, and thus the model is homoskedastic, i.e. all individuals are assumed to have the same error.

To fit the PPCA model to a dataset, we need to estimate the parameters of the model, namely $\mathbf{W}$ and $\Psi$ given the observed data. For complete data, $\Psi$ and $\mathbf{W}$ can be calculated from SVD using the maximum-likelihood estimators \cite{tipping_probabilistic_1999}:
\begin{align*}
    \hat{\Psi} &= \frac{1}{M-q}\sum_{j=q+1}^M e_{j}^2\\
    \hat{\mathbf{W}} &= \mathbf{U}(\mathbf{E}^2 - \hat{\Psi}\BI)^{1/2},
\end{align*}
where $e_j$ is called singular value, and is the $j$-th entry on the diagonal of $\mathbf{E}$.

Intuitively, the error term is just an average of all the eigenvalues that correspond to the eigenvectors that are thought of as contributing to the sampling noise, and therefore it is subtracted from the diagonal matrix of eigenvalues $\mathbf{E}$. Thus, the MLE solution of PPCA results differs from that of classical PCA only in that the first $q$ PCs are ``shrunk'' using a common term that incorporates the noise discarded in the PCA, and the last $M-q$ PCs are set to zero. Notably a PCA plot would look almost the same, only the axes scales would be different. Notably, for PPCA we need to set the number of retained dimensions $q$ a priori, since changing $q$ requires rescaling all PCs. In addition, setting $q=M-1$ (or equivalently, $\Psi=0$), recoups classical PCA.


\subsection{LSE and $F$-statistics}\label{theory-lse}
The drawback of PPCA is that it models the sampling error as the same for each sample, but in genetic data the error is binomially distributed. In addition, one needs to know $q$ a priori. LSE (Latent Subspace Estimation) is a dimensionality reduction technique that addresses these issues. LSE is quite similar to PPCA, with the difference that it accounts for the heteroscedasticity in the data \citep{chen_consistent_2015}, and explicitly models the binomial error in genetic data. The intuitive idea here is to calculate the sampling error for each sample assuming a binomial distribution, and to remove this error from the covariance matrix before doing SVD. So, in this algorithm, we calculate the heterozygosity  $d_{jj} = \frac{1}{S}\sum_{s=1}^S x_{js}(1 - x_{js})$ from $\MX$. we define $\mathbf{D}$ as a diagonal matrix with $j^{th}$ entry as $d_{jj}$. One can then estimate covariance matrix $\MG$ = $\frac{1}{S}\MX^T\MX - \mathbf{D}$. The eigenvectors of $\mathbf{G}$ then span the latent subspace of $\mathbf{L}$ , and the expected value of the smallest $M-q$ eigenvalues converge to 0 for large M \citep{cabreros_likelihood-free_2019}.


Crucially, if we use \emph{all} the PCs, the $f$-statistics coincide with LSE (Fig. \ref{figS4:lse_admix}, see the appendix for a derivation): 
\begin{align}
    f_2(X_1, X_4) &= \frac{1}{S} \sum_{s=1}^S (x_{1s} - x_{4s})^2 - d_{11} - d_{44} \nonumber\\
    &= G_{11} + G_{44} - 2 G_{14} \label{eq:lse}
\end{align}


The covariance used by LSE is an estimated covariance matrix \citep{van_waaij_evaluation_2023}, and its properties differ from the sample covariance matrix used in classical PCA. In particular, sample covariance matrices are positive semi-definite, which means that all eigenvalues are non-negative. This ensures that the eigenvalues represent the variance among the individuals along the corresponding PCs. In contrast, for an estimated covariance matrix, the expectation of the smallest $M-q$ eigenvalues is zero, and thus an unbiased estimate will have both positive and negative eigenvalues. Therefore, in case of LSE, we can calculate $F_2$ by adjusting eq. \ref{eq:f_pca} to include the sign of the eigenvalues:


\begin{align}\label{eq:f_lse}
F_2(X_1,X_4) &= \sum_{j=1}^{M}\mathbb{I}(e_j \geq 0) (w_{1j} - w_{4j})^2 - \sum_{j=1}^M \mathbb{I}(e_j <0) (w_{1j} - w_{4j})^2,
\end{align}
where $e_j$ denotes the $j$-th eigenvalue of $\mathbf{G}$, and $\mathbb{I}$ denotes the indicator function that is 1 when the condition is satisfied, and zero otherwise.


\subsection{Missing data}

It can be difficult to estimate $F$-statistics when there is high amount of missing data. A conservative approach to estimate individual-based $F$-statistics is to only retain sites where data is present from every single individual in the data set. However, even for moderately large data sets that quickly becomes prohibitive: As a toy example, consider a data set with 100 (haploid) individuals with 10\% missing data, and 1,000,000 sites. Out of those, only 26 are expected to be covered in every single individual, which makes this approach not feasible.

Thus, grouping individuals into populations can dramatically increase the number of sites retained. A common practice is to retain sites if at least one individual in each population has data. In the above example, if we grouped the 100 individuals into 10 populations of 10 individuals each, and retained sites where at least one individual in each population carried data, we would expect no missing sites in almost all cases. However, grouping individuals may not be justified when they do not form discrete clusters, or when there are very few samples whose population assignments are unknown.

Missing data is also a challenging problem while computing PCA, since SVD and eigendecompositions cannot be performed with missing data. One way to implement a PCA in case of missing data is to start with assigning the mean genotype to the missing values (mean imputation), and then perform PCA to reconstruct or impute the missing values, and keep iterating until convergence. This can be done both for classical PCA \citep{meisner_large-scale_2021} and for PPCA \citep{tipping_probabilistic_1999-1}.  

Another popular way to deal with missing data is to first construct a PCA using samples that have no missing data, and then project the low-coverage samples on this PCA \citep{patterson_population_2006, price_principal_2006}. Projection is a useful way to check if the low-coverage samples from the same population fall into the same position as the high coverage ones. In this approach, one needs to make sure that the construction of PCA is not affected by sampling bias (such as in the case of classical PCA). However, since sampling noise is independent for each projected sample, projection of samples on a constructed PCA does not require additional modeling of sampling bias. 

\section{Results}
In this section, we first describe the coalescent simulations we use to study the theoretical and statistical properties of different PCA methods. we compare the $F$-statistics predicted from different versions of PCA to those calculated using ADMIXTOOLS 2 \citep{maier_limits_2022}. Finally, we illustrate a practical application of our approach on a dataset of Neandertal genetic variation. we developed a snakemake pipeline \citep{molder_sustainable_2021} with python and R scripts for all the analyses described here (for details see section \ref{ch2_methods}).

\subsection{Evaluation on simulations}
I simulated 4 populations ($X_1$, $X_2$, $X_3$, $X_4$) with 11 individuals each and 6 populations ($X_5$,..,$X_{10}$) with 10 individuals each using slendr \citep{petr_slendr_2022}. In these simulations, populations $X_6$,..,$X_{10}$ were created from pairwise admixture events between populations $X_1$,..,$X_5$. In addition, we simulated a unidirectional gene flow from $X_3$ to $X_2$ with varying migration rates ($0\%, 1\%, 5\%$) (see Fig. \ref{fig2:sim}). we generated 20 simulations for each case of migration rate to evaluate our framework.  In these simulations, we used a mutation rate of $10^{-8}$ per base per generation, a recombination rate of $10^{-8}$ per base per generation, and a generation time of 30 years. 

\begin{figure}[ht!]
    \includegraphics[width=16.5cm]{Images/Figures/sim_final.png}
    \centering
    \caption{Overview of simulations. \textbf{A}. Schematic of the simulation. Numbers reflect split, migration and admixture times (in generations). Admixture events are shown in dashed lines, and the red arrow represents a unidirectional migration from $X_3$ to $X_2$. \textbf{B}. PPCA (with 8 PCs) of data simulated using the admixture graph depicted in A. Percentage labels show variation explained per PC. \textbf{C}. PPCA-biplots of the first 4 PCs.
}
    \label{fig2:sim}
\end{figure}


\paragraph{Interpretation of PCA of admixture graph:}
The admixture graph in Fig. \ref{fig2:sim} A shows a schematic of the population structure and demographic history for simulations. We can visualize this structure using two different PPCA plots: first we plot the first 8 PCs on the same scale (Fig. \ref{fig2:sim}B), and we also show the more standard PPCA biplots (Fig. \ref{fig2:sim}C). The genetic structure generated by the admixture graph is apparent in both visualizations: For example, the first PC mainly separates  the ``left'' clade (with $X_1$,$X_2$ and $X_{10}$), from the ``right'' clade ($X_3$,$X_4$ and $X_7$). The outgroup ($X_5$), is placed very close to zero here, which is because PC1 mainly reflects the variation between these clades, and the outgroup branch leading to $X_5$ is orthogonal to that variation. 

PC2, on the other hand, primarily separates out the outgroup ($X_5$) from the remaining samples, with the admixed population $X_6$ falling in between.

Thus, even though $X_5$ is the most genetically diverged population, this is not evident from PC1. The reason for this is that PCA maximizes the sum of all pairwise $F_2$-distances between individuals, and hence the ordering of PCs depends on sample configuration \citep{mcvean_genealogical_2009, elhaik_principal_2022}. In our case, there are only ten individuals in $X_5$, whereas we have a total of 84 individuals in the other populations (all except $X_6$, which has ancestry from $X_5$). In this case, the sum of the $\binom{84}{2}=3486$ pairwise $F_2$-distances within the clade is larger than the sum of the $84 \times 10 = 840$ distances between individuals from the clade to $X_5$. 

Next, PC 3 shows the axis of variation between $X_9$ and the other populations. Due to the drift on $X_9$ for many generations, it is hard to detect the signal for admixture ($f_3(X_9; X_1, X_4)$ = $0.0008$). In comparison, the admixture between $X_1$ and $X_2$ leading to $X_{10}$ is detectable ($f_3(X_{10}; X_1, X_2)$ = $-0.0002$). $f_3$ is calculated with the top 8 PCs in the above cases. The two next PCs, PC4 and PC5 mainly model the variation with the ``right'' clade, and PC6 splits up $X_1$ and $X_2$ within the ``left clade''. In all three of the PCs, populations outside the clades plot very close to zero, which is because in a tree, the within-clade variation is independent of that of the clade to individuals outside of it \citep{felsenstein_maximum-likelihood_1973}. 

Thus, we find that for tree and admixture graph models, most PCs tend to show variation within a single clade, and we need a fairly large number of PCs to tease out the full tree structure. This is consistent with the theoretical expectation that the covariance matrix for a (population) tree has full rank \citep{felsenstein_maximum-likelihood_1973}, i.e. we would expect the number of PCs required to reflect tree structure to be on the same scale as the number of populations \citep{patterson_population_2006}. 

PCA is most commonly visualized in biplots (Fig \ref{fig2:sim}C). A main advantage is that this provides a 2D-visualization of the high-level population structure (in our case, the separation of the outgroup from the ingroup samples and between the two main clades). However, we lose the structure within each clade, that is represented by PCs 3 and higher. Plotting a second biplot of PC3 vs PC4 is somewhat less informative, because the variation already explained by the higher PCs is absent.

In contrast, plotting PCs separately on one dimension has the advantage that the orthogonality of the PCs becomes more apparent, and also allows for easier quantitative comparison on how much each PC explains: The spread (i.e. variance explained) by each PC gets narrow and narrower as we move to higher PCs, and we can easily plot a large number of PCs. The drawback of this representation is that the correlation and 2D-structure in PC-space gets deemphasized. we plot the first 10 PCs from PPCA (scale=8), classical PCA and LSE (Fig. \ref{figS1:ppca_scale}, \ref{figS1:pca_scale}, \ref{figS1:lse_scale}), and find that the population structure represented by the PCs does not change. However, the PCs are scaled differently for different method, and so the absolute position of individuals varies. In particular, for PPCA (scale=8), all PCs $>8$ are scaled with zero, and hence PC 9 and PC 10 in this case look very different from that of classical PCA and LSE.     
It is interesting to note that a single PC is not informative. One needs to look at all the PCs to get a sense of the population structure. However, the information in the tree and the PC plot is the same, and so the tree can be regenerated from the (top) PCs. 

\paragraph{PCA-based $f_2$ estimates:}
I use eq. \ref{eq:f_pca} to write custom python scripts for calculating $f$-statistics via  classical PCA, PPCA and LSE, and to compare these $f$-statistics to the true values obtained from  the simulations. we implemented PPCA and LSE using custom R scripts, and classical PCA using prcomp function in R. 

To compare how the different $f_2$-estimates behave, we calculate $f_2(X_1, X_4)$  with an increasing number of PCs for the three methods (Fig. \ref{fig:comparison}).  

In the top panel of Fig. \ref{fig:comparison}, we use eleven diploid individuals per population. Since the sampling error is low with eleven individuals, the bias-correction is small, and the biased $f_2$-estimate (0.0866, calculated using eq. \ref{eq:f_intro}) is similar to the true value calculated from branch lengths (0.0845). In contrast, the bias is much larger when using a single diploid individual (middle/lower panel), with the biased estimate at 0.102. 

In either case, we find that classical PCA and PPCA both converge to the biased estimate when the full data is used, whereas LSE converges to the truth, as expected from eq. \ref{eq:lse}. However, the rates of convergence differ: For population-based samples, classical PCA needs just $\approx 7$ PCs to be very close to the biased estimate (0.0861), and the last 94 PCs only provide a marginal distribution. In contrast, in the individual-based analysis the estimate reaches a plateau between $7 \sim 23 PCs$ where its value is just a slight overestimate of the true value (0.0861 at 7 PCs, 0.0868 at 23 PCs).

For PPCA, we set the rank of the covariance matrix to the number of PCs, with the rest of the variation explained by the noise term. This greatly reduces the bias and the dependence on the number of PCs used: In the individual-based analysis $f_2 = 0.0841, 0.085, 0.0863$ using 7 PCs, 23 PCs and 51 PCs respectively, indicating that the PPCA-derived $f$-statistics are very robust to the number of PCs used. The third method, LSE, converges to the true value when all PCs are used. However, similar to PPCA we find that using 7, 23 or 51 PCs yields estimates with a minimal bias ($f_2 = 0.085, 0.0859, 0.0865$, respectively). It is interesting to note that the $f_2$ estimated with LSE initially increases with additional PCs, and then reduces to converge to the true value. This is because many small eigen values are negative and therefore $f_2$s calculated from the corresponding PCs are subtracted (see section \ref{compare_pca_discussion}).

In the final panel, we estimate $f_2$ with data where we introduce $50\%$ missingness (Fig. \ref{fig:comparison}). Implementing LSE with missing data is beyond the scope of this thesis, and we only compare PCA and PPCA. we again find that PPCA performs well over a fairly large range of number of PCs used but that the estimates become more erratic beyond 23 PCs ($f_2 = 0.0836, 0.0874, 0.1158$ using 7, 23 and 51 PCs respectively). In contrast, the estimates from classical PCA are poor throughout.

\begin{figure}[ht!]
    \includegraphics[width=16.5cm]{Images/Figures/mu0.05_main_fig_all_pca.png}
    \centering
    \caption{Comparison of PCA approaches using $f_2(X_1,X_4)$ estimated using 11 individuals for each population (top), 1 individual for each population (middle), and 1 individual for each population with $50\%$ missing genotypes (bottom). Dotted line represents biased estimate and dashed lines show the true value of $F_2$ from the simulations. Error bars represent 2 standard errors.}
    \label{fig:comparison}
\end{figure}

In the rest of the analyses (except analyses with missing data), we exclude PCA, and compare PPCA and LSE with 8 and 12 PCs to ADMIXTOOLS 2 \citep{maier_limits_2022}, which is a recent re-implementation of ADMIXTOOLS \citep{patterson_ancient_2012} that gives equivalent results. we chose 8 and 12 PCs because the number of PCs to use may not be known in most applications. we first compare $f_2$, $f_3$ and $f_4$ estimated by these methods from complete simulation data, with all individuals in each population, and no missing genotypes. In this case we find that all the methods perform well, and result in $F$-statistics that are very close to the truth both (Fig. \ref{figS5:comparison10}). 

I then subsample one individual from each population, and observe that in this case both PPCA and LSE- based frameworks perform atleast as well as ADMIXTOOLS 2 (Table S1) and the mean estimate from each method is close to the true value. However, the error bars for $f_2$ estimates (obtained using the point estimates from 20 simulations) are slightly lower for PPCA (scale=8) compared to those of ADMIXTOOLS 2: For $F_2(X_1,X_2)$, $F_2(X_3,X_4)$, $F_2(X_2,X_3)$ and $F_2(X_1,X_4)$, ADMIXTOOLS 2 standard errors are 0.0018, 0.0015, 0.0027, and 0.0012 while the PPCA and LSE-based estimates (scale=8) for the same statistics are 0.0012,0.0012,0.0024,0.0011 and 0.0012,0.0012,0.0025,0.0011 respectively). The improved accuracy of PCA-based tools versus ADMIXTOOLS 2 is explained because PCA incorporates a succinct summary of the full data of all the individuals, and thus the PCA-based estimates can ``borrow'' information from related individuals in the sample that are not used to calculate the statistic at hand. In contrast, ADMIXTOOLS 2 has only one individual from each population to assess structure / admixture, and while the estimates based on ADMIXTOOLS 2 are minimum-variance estimators for these subsets of the data \citep{patterson_ancient_2012}, PCA-based methods do better whenever we have data from additional individuals.


\subsection{Missing data}
Next, we address the issue of missing data and evaluate the estimation of $F$-statistics using one individual from each population with these methods when there is $50\%$ random missing data. Our implementation of PPCA with missing data is inspired from EMU \citep{meisner_large-scale_2021}, and is described in Methods section \ref{method-ppca}. However, the EMU implementation aims to reproduce classical PCA, and hence would result in biased estimates. Our correction is able to produce accurate $f$-statistics using PPCA (Fig. \ref{fig:comparison-adm}, Table S1, whereas both EMU and ADMIXTOOLS 2 estimates are inflated For the case of missing data, ADMIXTOOLS2 was run with maxmiss=1, so no SNPs are excluded. It was not possible to run ADMIXTOOLS 2 using sites with no missing data since with $50\%$ missing sites, very few sites are without missingness. 

\begin{figure}[ht!]
    \includegraphics[width=16.5cm]{Images/Figures/mu0.05_plot_all_1ind_missing.png}
    \centering
    \caption{Comparison of PPCA and classical PCA to ADMIXTOOLS 2 in the presence of $50\%$ random missingness, using genotypes from one individual from each population. Red dashed line reflects the true value. Error bars represent 2 standard errors.}
    \label{fig:comparison-adm}
\end{figure}

\subsection{Test of admixture}
A major application of $F$-statistics are tests of admixture \citep{orlando_ancient_2021}. we showed in the previous section that PPCA framework can be used to calculate the point estimates of $F$-statistics. In this section we show that we can also get standard errors for these estimates using block-jackknife \citep{kunsch_jackknife_1989,maier_limits_2022, patterson_modication_2020}, and use these to do hypothesis testing for admixture. we simulate a gene flow from $X_3$ to $X_2$ 500 generations ago with the migration rate of $\mu \epsilon [0, 0.01, 0.05]$. we then compare the estimates of $F$-statistics from PPCA framework to that from ADMIXTOOLS 2. we first test for admixture by checking if the estimate of $F_4(X_1,X_2;X_3,X_4)$ is significantly different from 0. we show that when there are 11 individuals in each population, both methods perform well (Fig. \ref{figS7:test}). In case of 0 migration rate, both methods estimate $F_4$ for all simulations to be close to 0, while at $5\%$ migration rate, ADMIXTOOLS 2 and PPCA-framework have the power to detect admixture (with $F_4$ estimate 2 standard deviations below 0) in $90\%$ and $70\%$ simulations respectively. At migration rate of $1\%$, both the methods are unable to find admixture between $X_2$ and $X_3$, and instead incorrectly predict admixture between $X_1$ and $X_3$ ($F_4$ estimate is 2 standard deviations above the mean) for one simulation. Reducing the number of individuals to 1 from each population and adding $50\%$ missingness reduces the power for both the methods. In this case, both methods have no false positives in case of 0 migration rate, and at $5\%$ migration rate ADMIXTOOLS 2 and PPCA framework detect admixture in $5\%$ and $35\%$ simulations respectively. At $1\%$ migration rate, ADMIXTOOLS 2 infers admixture between $X_2$ and $X_3$ in 2 simulations and incorrect admixture between $X_1$ and $X_3$ in one simulation, while PPCA framework shows no prediction of admixture.

\begin{figure}[ht!]
    \includegraphics[width=16.5cm]{Images/Figures/hypothesis_test_comparison.png}
    \centering
         \caption{Test for admixture with individual-based $f_4(X_1, X_2, X_3, X_4)$ statistic. we compare ADMIXTOOLS 2 (orange) to PPCA-based-framework (blue) using 20 simulations with $50\%$ missing genotypes. Jitter is added on x-axis for visual clarity.}
    \label{fig:admixture}
\end{figure}

\subsection{Evaluation on Neandertal dataset}

To test our framework on real data, we use it to re-analyze a published dataset of archaic humans from Eurasia \citep{hajdinjak_reconstructing_2018}. This dataset consists of low-coverage sequence data from late Neandertals from Western Eurasia, which were included as pseudo-haploid genotypes. For context, the authors also added the high-coverage diploid genotype sequences from Vindija cave, Croatia (Vindija33.19)\citep{prufer_high-coverage_2017} and Densiova Cave, Russia(Altai, Denisova)\citep{prufer_complete_2014, meyer_high-coverage_2012}. The Denisova specimen (\textit{Denisova 3}) was the first Denisovan discovered, whereas the Altai Neandertal (\textit{Denisova 5}) is a Neandertal from the same site that has been shown to be on a different branch than all the late Neandertals \citep{prufer_complete_2014, prufer_high-coverage_2017}. we find that the results of PPCA and classical PCA with projection, as implemented in smartPCA (with only two PCs to make our analysis same as the original study), provide a visually very similar PCA plot (Fig. \ref{fig:nea_f3}). In both cases, PC1 separates out the Denisovan from the Neandertals, whereas PC2 results in a gradient from Altai to \textit{Vindija 33.19}. 
The main difference is that because classical PCA includes sampling noise in the analysis, it overestimates the absolute magnitude of differentiation; classical PCA visualizes the biased estimator for $F_2$, wheras PPCA approximates the unbiased estimator. The $\%$ variation in Fig. \ref{fig:nea_f3}A is the normalized eigenvalue from each PC from smartpca, whereas the $\%$ variation in \ref{fig:nea_f3}B represents the proportion of variation explained by population structure. The variances explained by PC1 and PC2 are high and add up to $100\%$ in both A and B since in both cases, since only 2 PCs were utilized.

I next compare how $f$-statistics computed using PPCA and ADMIXTOOLS 2 differ from each other (Fig \ref{fig:nea_f3}C). For this purpose, we focus on an outgroup-$F_3$ statistic $F_3$(Altai; Vindija33.19, X). This statistic measures the similarity of Neandertal X to the high-coverage Vindija Neandertal, using the Altai Neandertal as an outgroup. More precisely, we project the low-coverage samples onto the line joining Vindija33.19 and Altai in Fig. \ref{fig:nea_f3}B, and measure the distance from the low-coverage sample to the Altai Neandertal; thus higher values of $f_3$ denote higher similarity of X to Vindija33.19 (Fig. \ref{fig2:overview}). 

Overall, we find the resulting pattern to be similar between the methods, but the PPCA-based values are consistently larger than those obtained using ADMIXTOOLS2. To further investigate this discrepancy, we focus on Vindia 87, which has been shown to come from the same individual as Vindija 33.19 \citep{hajdinjak_reconstructing_2018}. we decompose  $F_3$ as a linear combination of $F_2$s:

\begin{dmath}
    f_3(Altai; Vindija33.19, Vindija 87) = f_2(Altai, Vindija33.19) + f_2(Altai, Vindija 87) - f_2(Vindija33.19, Vindija 87)\\
\end{dmath}

and analyze them seperately for ADMIXTOOLS2 and PPCA.
Using ADMIXTOOLS 2, we found that $f_2$(Altai, Vindija33.19) and $f_2$(Altai, Vindija 87) have values 0.072 and 0.135,  respectively. Since both Vindija samples are from the same individual, the $f_2$-values are expected to be the identical. In contrast, using the PPCA framework, we obtain the values of $f_2$(Altai, Vindija33.19)=0.102 and $f_2$(Altai, Vindija 87)=0.115, respectively, which are much closer. In addition, $f_2$ should be 0 for two samples from the same individual (i.e. Vindija33.19, Vindija 87). ADMIXTOOLS 2 yields a value of 0.0057, and the PPCA-framework results in a smaller value of 0.00094. 

One explanation for this is that Vindija 87 is pseudohaploid, and the unbiased estimator in ADMIXTOOLS 2 for $F_2$ is undefined for single pseudohaploid genomes ($n_{1s}=1$ in eq. \ref{eq:f2_correction}), and so ADMIXTOOLS2 defaults to a biased estimator. In contrast, the PPCA-based estimator is still defined and can be used in this case, but it introduces the assumption that the samples have similar heterozygosities. 

\begin{figure}[ht!]
    \includegraphics[width=16.5cm]{Images/Figures/nea_main.png}
    \centering
    \caption{\textbf{A}. PCA of archaic neandertals created with smartpca with high coverage neandertals, and projection of the rest. we use  two PCs because we just have three high-coverage samples.
    \textbf{B}. PPCA of archaic specimens with scale=2. In A and B, The $\%$ variance on x and y-axes denotes the $\%$ variance explained by the population structure. \textbf{C}. $F_3$(Altai, Vindija33.19, X) estimated with ADMIXTOOLS 2 and PPCA-based-framework. Larger value on x-axis represents more proximity to Vindija33.19. Bars show 2 standard errors.}
    \label{fig:nea_f3}
\end{figure}


\section{Methods}\label{ch2_methods}

\subsection{PPCA implimentation}\label{method-ppca}

I implement PPCA using maximum-likelihood approach following \cite{tipping_probabilistic_1999-1}, and modify this algorithm to work with missing data. Our approach to handle missingness is inspired from EMU \citep{meisner_large-scale_2021}. we describe our algorithm briefly:
\begin{enumerate}
    \item Mean center data $\MX_c = \MX - \mu$.
    \item Set missing values to 0.
    \item Perform SVD of $\MX_c$, $\MX_c = \mathbf{UEV}^T$.
    \item Calculate the MLE of the Gaussian noise parameter $\Psi = \frac{1}{M-q} \sum_{j=M-q}^ M e_j^2$ as the average of square of the $M-q$ smallest singular values.
    \item Obtain the MLE of the $q^{th}$ eigenvalue as $e_q^2 - \Psi$.
    \item Calculate the linear mapping matrix $\BW = \mathbf{U} (\mathbf{E}^2 - \Psi\BI)^{1/2}$.
    \item Reconstruct mean-centered data: $\mathbf{X_R} = \BW(\BW^T\BW)^{-1}\BW^T\MX_c$.
    \item Replace missing value with reconstructed values.
    \item Repeat steps 2-8 until convergence.
\end{enumerate}

Our algorithm differs from that of EMU at steps 4-7, which deal with the modelling of the sampling noise and reconstruction of data, and are specific to PPCA algorithm.

\subsection{Calculation of standard errors}

I use a block-jackknife approach to calculate standard errors \citep{kunsch_jackknife_1989, maier_limits_2022, patterson_modication_2020}. we divide the genome in 2 MB blocks, and estimate PCs and $F$-statistics utilizing the entire genotype matrix except for one block. we repeat this procedure for all the blocks. Since the statistics obtained in this way are not independent, we calculate variance similar to ADMIXTOOLS 2 using 

\begin{align}\label{eq:bjk_var}
V &= \frac{1}{g} \sum_{i=1}^g \frac{s_i}{S-s_i} (\hat{\theta} - \theta_i)^2
\end{align}

Here, V is the variance of a statistic $\theta$, $\hat{\theta}$ is the average estimate of the statistic, and $\theta_i$ is the estimate with block $i$ removed. Here $g$ denotes the number of blocks, $s_i$ the number of sites in block $i$, and $S$ is the total number of sites.

\subsection{Simulation pipeline}

I used slendr \citep{petr_slendr_2022} to simulate 4 populations with 11 individuals each and 6 populations with 10 individuals each. we used a mutation rate of $10^{-8}$ per base per generation, a recombination rate of $10^{-8}$ per base per generation, and a generation time of 30 years. Five of these populations were created by admixing other populations to create a complex scenario. Additionally, we created a geneflow event from $X_3$ to $X_2$ 500 generations ago (Fig. \ref{fig2:overview}). we calculated true $F$-statistics from the simulated samples using the branch lengths of the trees, using the functions implemented in \texttt{tskit} \citep{baumdicker_efficient_2022}. we used eigenstrat files as input for all PCA methods and ADMIXTOOLS 2. 

\section{Discussion}
In this study, we present a statistical framework to jointly compute PCA and $F$-statistics. Many ancient genetic studies use both of these tools, but make slightly different assumptions, slightly different models and different software for them. In contrast, our joint framework allows us to make sure that assumptions are consistent throughout the analysis. The key advantage is that the effect of modelling assumptions becomes apparent, and this also allows us to make novel recommendations about how PCA-based analyses should be performed and interpreted.

The connections between $F$-statistics and PCA allow us to provide a better understanding on how different PCA algorithms emphasize different parts of the data, and how they emphasize population structure versus sampling noise.

In particular, $F$-statistics enable quantitative interpretation of PCA-plots, where distances on a PCA are directly proportional to genetic differentiation, and orthogonal projections can directly be used to test for admixture.
However, the above statement is true only when all the (relevant) PCs are used. Visualizing two or only few PCs may be insufficient to accurately visualize population structure and admixture. 

\subsubsection{Interpreting PCA plots}
There is a considerable literature aimed at interpreting PCA-plots, which generally fall in two approaches: First, for simple models, such as lattice models \citep{novembre_interpreting_2008} and isolation-without-migration models \citep{mcvean_genealogical_2009}, the eigenvectors, and hence the PCs, can be calculated analytically. These approaches are powerful to understand the ``inner workings'' of PCA, but do not deal with the variation and noise inherent in sample data. Thus, much more common are interpretations based on empirical observations, such as that the first PC commonly aligns with the axis of an expansion \citep{l_l_cavalli-sforza_history_1996}, that PCs tend to align with the branches of a population tree or that the first two PCs recap a map of Europe \citep{novembre_genes_2008, cavalli-sforza_analysis_1975}.

However, these observational guidelines have provided to not hold particularly well in simulations, and have been shown to be strongly influenced by sampling schemes, simulation details and other factors \citep{novembre_interpreting_2008, degiorgio_geographic_2013, elhaik_principal_2022, jay_anisotropic_2013} enough that interpretations of the first few PCs in terms of demographic history are frequently discouraged.

Our link between PCA and $F$-statistics provides a different way of interpreting PCA-plot, by studying it in terms of the embedded $F$-statistics. A key difference is that our approach is quantitative, and provides exact results for all PCs, and good approximations when only the first few PCs are used \citep{peter_geometric_2022}. Then, $F$-statistics can directly be calculated from one-dimensional PCA-plots such as that in Figure \ref{fig2:sim}B: $F_2(X_1, X_2)$ corresponds to the (squared) sum of the distance in that figure, and we can see from the plot that PC6 is the main PC that teases out that axis of variation (because the two populations are furthest apart).  
The drawback of our approach is that $F$-statistics are not directly interpretable in terms of demographic history, and we need additional steps to link them with what we are typically interested in. 

\subsubsection{Comparing PCA methods}\label{compare_pca_discussion}
Different PCA methods we discussed here differ in how they deal with biological vs. sampling variation, and how different sources of variations are emphasized. This gives us some insights for when different PCA algorithms should be used. 

Classical PCA is mathematically the simplest, and still the most widely used method for visualizing population structure. It has an interpretation in terms of pairwise coalescent times \citep{mcvean_genealogical_2009}. However, because classical PCA incorporates \emph{all} variation in the data, it does not distinguish between variation due to population structure, and variation due to sampling. This can lead to problems: For example, one cannot directly project additional samples to a PCA, instead some correction is required otherwise some ``shrinkage'' occurs where new samples are projected closer to the origin \citep{patterson_population_2006, wang_improved_2015}. Thus, in our opinion classical PCA should be the primary choice for analyses primarily aimed at quality control, because incorporating noise allows it to reveal technical artifacts and outliers. Effects of different sequencing or capturing techniques will very often be visible on a PCA-plot, while they may be more hidden and only partially corrected in PPCA.

Our results suggest that the methods that separate sampling noise from population structure, such as PPCA and LSE, are preferable to classical PCA when the primary goal is to depict population structure. PPCA is the simpler of the two methods, because it assumes homoskedastic noise, i.e. that all samples have the same heterozygosity or sampling variance. As \cite{tipping_probabilistic_1999-1} showed, without missing data, the maximum-likelihood estimator of PPCA results in virtually identical PCA plots compared to those obtained from a classical PCA; the only differences are that the axes for the first few PCs will be re-scaled, and that the majority of PCs are set to zero (Fig. \ref{figS1:ppca_scale}, Fig. \ref{figS1:pca_scale}).


LSE goes one step further by modelling heteroskedastic, binomial noise. This ensures that both PCA and $F$-statistics use the exact same modelling assumptions, and thus LSE-based PCA plots are directly comparable to $F$-statistics (Fig. \ref{figS4:lse_admix}). This relationship is exact if all PCs are used. One advantage of LSE is that the eigenvalues corresponding to the PCs irrelevant for population variation have an expectation of zero. Since these eigenvalues are directly proportional to the amount of variance explained per PC, and to the contribution to $F$-statistics, we expect that truncating them will yield good results, which is indeed what we see in simulation results (Fig. \ref{figS1:lse_scale}). 

In real data, this expectation of zero for the eigenvalues will typically yield both positive and negative eigenvalues, which is why the equation for computing $f_2$ needs to be adjusted (eq. \ref{eq:f_lse}), and subtract the contribution to $F_2$ by PCs that have negative eigenvalues.

From a theoretical point of view, the model optimized in LSE is more desirable to that in PPCA, because it takes into account that different individuals might have different heterozygosities. However, the advantage of PPCA is that it is relatively easier to implement with missing data,even for single pseudohaploid samples, which are common in ancient DNA applications (Fig. \ref{fig:comparison-adm}) \citep{tipping_probabilistic_1999-1, orlando_ancient_2021}. 


A further approach that is used for ancient DNA is to project low-coverage data onto a ``reference''-data set. This has three useful advantages: First, the overall shape of the PCA only depends on the reference data set. Thus, PCA-plots using the same reference data become directly comparable, which can be useful as a quick way of assessing population variation, e.g. using the Western Eurasian PCA used in studies of Holocene ancient DNA studies from Europe and Western Asia \citep{haak_massive_2015}. Second, it deals with missing data in the projected samples, because often only a subset of sites are required for an accurate projection. Third, it also deals with sampling noise in the projected sample, because the sampling noise is orthogonal to the variation in the reference data set, and thus gets removed by the projection.

However, the drawback of using projections, and why they, in our opinion, are inferior to probabilistic PCA, is that they do not capture the full variation in the data. In particular, only the variation in the reference data is considered, but not the variation that is private to the projected samples. 

In terms of $F$-statistics \citep{peter_geometric_2022}, 

\begin{equation*}
    F_2(X_1, X_4) = \underbrace{F_2^{\text{(PCA)}}(X_1, X_4)}_{\text{visible in projection PCA}} + \underbrace{F_2^{\text{(Projection error)}}(X_1, X_4)}_{\text{hidden in projection PCA}}
\end{equation*}

This is particularly problematic when projecting ancient human samples onto a modern reference data set, because the differentiation between ancient human population is often considerably larger than that between present-day populations \citep{haak_massive_2015, lazaridis_ancient_2014}, and thus projection PCA have no quantitative interpretation.


\subsubsection{Plotting PCs}
One of the major benefits of using $F$-statistics and PCA in a joint framework is that it enables a quantitative interpretation of PCA. Thus, our results directly suggest some recommendations for how PCs should be plotted when displaying population genetic variation is desired. 

First, the scale at which PCs are plotted matters, and the units on the different PCs should be comparable (Figure \ref{fig2:sim}B). Thus, it is recommended that PCA-biplots should be plotted with ``fixed'' aspect ratios where the x- and y-axes have the same units and scale, contrary to the current common practice where the axes are scaled arbitrarily (e.g. \cite{novembre_genes_2008, peter_genetic_2020}). It also highlights the benefits of plotting PCs on a shared axis such as in Figure \ref{fig2:sim}B, where the decreased variance explained by each subsequent PC is directly visualized, and tens of PCs can be plotted jointly.

Second, it has been a ``best-practices'' suggestion that PCs are plotted along with the proportion of variance explained by each PC (e.g.\citep{novembre_recent_2016, elhaik_principal_2022}). While we think this holds, it is important to recognize that different PCA algorithms handle this differently: In classical PCA, the proportion of variance is out of the total variance, including population structure, within-population variation and sampling noise. In contrast, in PPCA the number of PCs retained is a parameter, which \emph{sets} the proportion of variance explained by population structure. Thus, we obtain the (user-controlled) proportion of variance due to noise (corresponding to $\Psi$), and the proportion of variance each PC explains as a proportion of the variation explained by population structure. For LSE, the proportion of variance is calculated from the variance contributing to the population structure. In addition, the proportion of variance explained from a projected PCA will depend on the number of PCs or eigenvectors used for reconstruction of projected data \citep{patterson_population_2006}. 
Thus, the percent of variation explained between classical PCA, PPCA, LSE and projected PCA are not directly comparable.

\subsubsection{Grouping individuals}
A benefit of the interpretation that we developed is that it demonstrates when and how we can use PCA to group individuals into populations for $F$-statistics-based analysis. 
Thus, the groupings of individuals into populations becomes a result of the analysis, as opposed to an \textit{a priori} assumption set by the researcher.

This \textit{a priori} grouping may lead to difficulties in analyses, for example when dealing with recent migrants, or when grouping sparsely sampled populations \citep{shringarpure_effects_2014}. As ancient genetic sampling becomes more dense, recent migrants have become increasingly prevalent in data sets. These migrants are important, because they allow tracking of movements -- often over large distances -- within the last handful of generations. Thus, these recent migrants have become an important focus of study and it is desirable to distinguish recent migration from the more distant-in-time migration that is ordinarily the focus of $F$-statistic analyses.

A PCA will reveal such recent migrants as outliers of that group with their population-of-origin, whereas they would be missed by $F$-statistics.

In sparsely sampled populations, it is often necessary to group individuals that span relatively large areas or times to get to a usable sample size, particularly for regions where DNA preservation is poor. However, grouping genetically distinct individuals can be risky, as population substructure can invalidate the interpretation of $F$-statistic-tests \citep{peter_admixture_2016}. However, PCA provides a meaningful and simple way to assure that populations are devoid of relevant substructure: If individuals  cluster tightly on \emph{all} PCs that are meaningful for between-cluster variation, they can safely be grouped. However, if there is within-population variation that is correlated with other populations, then $F$-statistic-tests cannot be used to test for admixture. 

As discussed above, these considerations hold particularly when using a PCA method that separates sampling noise from population structure, such PPCA or LSE. For classical PCA, clustering will also be impacted by sample quality and depth - which can be separated out effectively by the probabilistic methods.



\subsubsection{Handling missing data}
Incomplete genotype calls with missing data are common in ancient DNA, and thus it is important that methods are able to deal with it. Classical PCA, which is based on SVD, relies on complete data, although methods that impute data are becoming more common \citep{meisner_large-scale_2021}. For $F$-statistics, ADMIXTOOLS2 has a parameter \texttt{maxmiss} that controls how much missingness is permitted, e.g. \texttt{maxmiss=0} removes all sites with any missing data, and \texttt{maxmiss=1} retains all the sites with any data.

For our tests with missingness, we could not use the option maxmiss=0, since this resulted in no sites available. Hence, we used \texttt{maxmiss=1} throughout, which resulted in inflated $F$-statistics in our simulations with $50\%$ missing genotype calls. In contrast, the PPCA-based algorithm is not affected, and appears to be more robust to missingness for both 8 and 12 PCs. It may seem surprising that ADMIXTOOLS 2 shows inflated estimates of $F$- statistics, given that it is based on Patterson's unbiased estimator. The reason for this inflation is that in the analysis for missing data we use one individual from each population, and when there is missing data, it results in only haploid genotype. In such cases, the correction term for the unbiased estimator is undefined. Thus we show that ,PPCA provides a practical solution in case of estimation of individual-based $F$-statistics with missing data. 

\subsubsection{Application on real data}
I used a PPCA-based framework with Neandertal data to estimate PCs utilizing all the samples. This approach is more straight-forward than the authors' approach to first estimate the PCs using high-coverage genomes, and then project the low-coverage genomes. we show that one can accurately estimate outgroup $F_3$ statistic from the PCs. we find that $F_2$ calculated using diploid genotype data with our framework is comparable to that of ADMIXTOOLS 2. However, ADMIXTOOLS 2 can not be used to estimate $F_2$ with pseudohaploid data since the unbiased estimator in ADMIXTOOLS 2 is undefined in this case. we show that PPCA-based framework provides accurate estimates even with pseudohaploid samples. Here, we point out that the reason PPCA performs well in this case is because the scalar noise parameter $\psi$ is determined from all the samples.

\subsubsection{Limitations and future directions}
One limitation of this framework is the need to perform multiple PPCAs to obtain standard errors, which can be computationally expensive. Further studies are needed to design a statistical framework that can estimate the errors using SNP loadings, and therefore can work fast with large datasets. In case of PPCA, one needs to determine the number of PCs relevant for the particular analysis. we show that PPCA is not sensitive to small changes in the number of PCs, and we provide the option of LSE which does not require user-defined number of PCs. Finally, for LSE, an algotithm to deal with missing data is still pending. 

\subsubsection{Summary}
To summarize, we present a method to perform PCA and $F$-statistics jointly and show that this approach not only improves estimates of $F$-statistics, but also provides a solution to the standardization and quantification of PCA. Our framework is available on github as a snakemake pipeline: https://github.com/DivyaratanPopli/A-joint-framework-for-PCA-and-$F$-statistics.

\bibliographystyle{evo3}

\bibliography{references.bib}

\end{document}